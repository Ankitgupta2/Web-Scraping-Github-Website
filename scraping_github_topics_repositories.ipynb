{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "scraping-github-topics-repositories",
      "provenance": [],
      "authorship_tag": "ABX9TyON9QrgxrNZihZp0cpxHrpr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ankitgupta2/Web-Scraping-Github-Website/blob/main/scraping_github_topics_repositories.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PmeW_tTyn0Fa"
      },
      "source": [
        "Scraping the top repositories for Topic on GitHub\n",
        "\n",
        "TODO (Intro) :\n",
        "- Introduction about web scraping: Web scraping, web harvesting, or web data extraction is data scraping used for extracting data from websites. The web scraping software may directly access the World Wide Web using the Hypertext Transfer Protocol or a web browser. \n",
        "-  It is a form of copying in which specific data is gathered and copied from the web, typically into a central local database or spreadsheet, for later retrieval or analysis.\n",
        "- Introduction about GitHub :  GitHub, Inc. is a provider of Internet hosting for software development and version control using Git. It offers the distributed version control and source code management (SCM) functionality of Git, plus its own features. \n",
        "- Tools used (Python,requests,Beautiful Souo,Pandas)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YtqBIR7Eo5Dy"
      },
      "source": [
        "Scrap the list of Topics From GitHub\n",
        "- use requests to download the page\n",
        "- use BS4 to parse and extract information\n",
        "- convert to pandas DataFrame"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x8nLC3OdqpV3"
      },
      "source": [
        "Lets write a function to download the page"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V2629-3crh0t"
      },
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "import pandas as pd\n",
        "import os"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UgOGyxRtohoM"
      },
      "source": [
        "def get_topic_page(topic_urls):\n",
        "   #download the page\n",
        "  response=requests.get(topic_urls)\n",
        "  #check sucessfull response\n",
        "  if response.status_code!=200:\n",
        "    raise Exception('failed to load page{}'.format(topic_urls))\n",
        "  #parse using beautiful soup\n",
        "  topic_doc=BeautifulSoup(response.text,'html.parser')\n",
        "  return topic_doc\n",
        "def parse_star_count(stars_str):\n",
        "  stars_str=stars_str.strip()\n",
        "  if stars_str[-1]=='k':\n",
        "    return int(float(stars_str[:-1])*1000)\n",
        "  return int(stars_str)\n",
        "def get_repo_info(repo_tags,repo_stars):\n",
        "  a_tags=repo_tags.find_all('a')\n",
        "  username=a_tags[0].text.strip()\n",
        "  repo_name=a_tags[1].text.strip()\n",
        "  repo_url='https://github.com'+a_tags[1]['href']\n",
        "  stars=parse_star_count(repo_stars.text.strip())\n",
        "  return username, repo_name, stars, repo_url\n",
        "\n",
        "def get_topic_repos(topic_doc):\n",
        "  #get h3 tag\n",
        "  h3_selection_class='f3 color-text-secondary text-normal lh-condensed'\n",
        "  repo_tags=topic_doc.find_all('h3',{'class':h3_selection_class})\n",
        "  #get star tags\n",
        "  star=\"social-count float-none\"\n",
        "  repo_stars=topic_doc.find_all('a',{'class':star})\n",
        "  repo_topics_dict={\n",
        "    'username':[],\n",
        "    'repo_name':[],\n",
        "    'stars':[],\n",
        "    'repo_url':[]\n",
        "  }\n",
        "  #get repo info\n",
        "  for i in range(len(repo_tags)):\n",
        "      repo_info=get_repo_info(repo_tags[i],repo_stars[i])\n",
        "      repo_topics_dict['username'].append(repo_info[0])\n",
        "      repo_topics_dict['repo_name'].append(repo_info[1])\n",
        "      repo_topics_dict['stars'].append(repo_info[2])\n",
        "      repo_topics_dict['repo_url'].append(repo_info[3])\n",
        "\n",
        "  return pd.DataFrame(repo_topics_dict)\n",
        "import os\n",
        "def scrape_topic(topic_url,path):\n",
        "  if os.path.exists(path):\n",
        "    print('The file {} already exists. skipping  ...'.format(path))\n",
        "    return\n",
        "  topic_df=get_topic_repos(get_topic_page(topic_url))\n",
        "  topic_df.to_csv(path,index=None)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3a7AT6fNq-6B"
      },
      "source": [
        "def get_topic_titles(doc):\n",
        "  selection_class='f3 lh-condensed mb-0 mt-1 Link--primary'\n",
        "  topic_title_tags=doc.find_all('p',{'class':selection_class})\n",
        "  topic_titles=[]\n",
        "  for tag in topic_title_tags:\n",
        "    y=tag.text\n",
        "    topic_titles.append(y)\n",
        "  return topic_titles\n",
        "\n",
        "def get_topic_descs(doc):\n",
        "  desc_selector='f5 color-text-secondary mb-0 mt-1'\n",
        "  topic_desc_tags=doc.find_all('p',{'class':desc_selector})\n",
        "  topic_descs=[]\n",
        "  for tag in topic_desc_tags:\n",
        "    y=tag.text.strip()\n",
        "    topic_descs.append(y)\n",
        "  return topic_descs\n",
        "\n",
        "def get_topic_urls(doc):\n",
        "  topic_links_selector='d-flex no-underline'\n",
        "  topic_link_tags=doc.find_all('a',{'class':topic_links_selector})\n",
        "  topic_urls=[]\n",
        "  base_url='https://github.com'\n",
        "  for tag in topic_link_tags:\n",
        "    y=base_url+tag['href']\n",
        "    topic_urls.append(y)\n",
        "  return  topic_urls\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def scrape_topics():\n",
        "  topics_url='https://github.com/topics'\n",
        "  response=requests.get(topics_url)\n",
        "  if response.status_code!=200:\n",
        "      raise Exception('failed to load page{}'.format(topic_urls))\n",
        "  doc=BeautifulSoup(response.text,'html.parser')\n",
        "  topics_dict={\n",
        "      'title':get_topic_titles(doc),\n",
        "      'description':get_topic_descs(doc),\n",
        "      'url':get_topic_urls(doc)\n",
        "  }\n",
        "  return pd.DataFrame(topics_dict)\n",
        "  "
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qIwosJZ4rFOz"
      },
      "source": [
        "def scrape_topics_repos():\n",
        "  print('Scraping Top Topics from github')\n",
        "  topics_df=scrape_topics()\n",
        "  #create folder\n",
        "  os.makedirs('data',exist_ok=True)\n",
        "  for index, row in topics_df.iterrows():\n",
        "    print('Scraping Top Repositories for \"{}\"'.format(row['title']))\n",
        "    scrape_topic(row['url'],'data/{}.csv'.format(row['title']))"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QF0b7u-lrStH",
        "outputId": "99b073b9-0d82-4ebe-e107-1d47867063dc"
      },
      "source": [
        "scrape_topics_repos()"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Scraping Top Topics from github\n",
            "Scraping Top Repositories for \"3D\"\n",
            "The file data/3D.csv already exists. skipping  ...\n",
            "Scraping Top Repositories for \"Ajax\"\n",
            "The file data/Ajax.csv already exists. skipping  ...\n",
            "Scraping Top Repositories for \"Algorithm\"\n",
            "The file data/Algorithm.csv already exists. skipping  ...\n",
            "Scraping Top Repositories for \"Amp\"\n",
            "The file data/Amp.csv already exists. skipping  ...\n",
            "Scraping Top Repositories for \"Android\"\n",
            "The file data/Android.csv already exists. skipping  ...\n",
            "Scraping Top Repositories for \"Angular\"\n",
            "The file data/Angular.csv already exists. skipping  ...\n",
            "Scraping Top Repositories for \"Ansible\"\n",
            "The file data/Ansible.csv already exists. skipping  ...\n",
            "Scraping Top Repositories for \"API\"\n",
            "The file data/API.csv already exists. skipping  ...\n",
            "Scraping Top Repositories for \"Arduino\"\n",
            "The file data/Arduino.csv already exists. skipping  ...\n",
            "Scraping Top Repositories for \"ASP.NET\"\n",
            "The file data/ASP.NET.csv already exists. skipping  ...\n",
            "Scraping Top Repositories for \"Atom\"\n",
            "The file data/Atom.csv already exists. skipping  ...\n",
            "Scraping Top Repositories for \"Awesome Lists\"\n",
            "The file data/Awesome Lists.csv already exists. skipping  ...\n",
            "Scraping Top Repositories for \"Amazon Web Services\"\n",
            "The file data/Amazon Web Services.csv already exists. skipping  ...\n",
            "Scraping Top Repositories for \"Azure\"\n",
            "The file data/Azure.csv already exists. skipping  ...\n",
            "Scraping Top Repositories for \"Babel\"\n",
            "The file data/Babel.csv already exists. skipping  ...\n",
            "Scraping Top Repositories for \"Bash\"\n",
            "The file data/Bash.csv already exists. skipping  ...\n",
            "Scraping Top Repositories for \"Bitcoin\"\n",
            "The file data/Bitcoin.csv already exists. skipping  ...\n",
            "Scraping Top Repositories for \"Bootstrap\"\n",
            "The file data/Bootstrap.csv already exists. skipping  ...\n",
            "Scraping Top Repositories for \"Bot\"\n",
            "The file data/Bot.csv already exists. skipping  ...\n",
            "Scraping Top Repositories for \"C\"\n",
            "The file data/C.csv already exists. skipping  ...\n",
            "Scraping Top Repositories for \"Chrome\"\n",
            "The file data/Chrome.csv already exists. skipping  ...\n",
            "Scraping Top Repositories for \"Chrome extension\"\n",
            "The file data/Chrome extension.csv already exists. skipping  ...\n",
            "Scraping Top Repositories for \"Command line interface\"\n",
            "Scraping Top Repositories for \"Clojure\"\n",
            "Scraping Top Repositories for \"Code quality\"\n",
            "Scraping Top Repositories for \"Code review\"\n",
            "Scraping Top Repositories for \"Compiler\"\n",
            "Scraping Top Repositories for \"Continuous integration\"\n",
            "Scraping Top Repositories for \"COVID-19\"\n",
            "Scraping Top Repositories for \"C++\"\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}